{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Latent Feature Representation\n",
    "\n",
    "Idea: Train an Autoencoder network to generate a latent feature representation\n",
    "for the heartbeats, then use this representation with a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, optimizers\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "\n",
    "     if dataset == 'mitbih':\n",
    "         df_train = pd.read_csv(\"../data/mitbih_train.csv\", header=None)\n",
    "         df_train = df_train.sample(frac=1)\n",
    "         df_test = pd.read_csv(\"../data/mitbih_test.csv\", header=None)\n",
    "\n",
    "         Y = np.array(df_train[187].values).astype(np.int8)\n",
    "         X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "         Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "         X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "         return X,Y,X_test,Y_test\n",
    "\n",
    "     elif dataset == 'ptbdb':\n",
    "         df_1 = pd.read_csv(\"../data/ptbdb_normal.csv\", header=None)\n",
    "         df_2 = pd.read_csv(\"../data/ptbdb_abnormal.csv\", header=None)\n",
    "         df = pd.concat([df_1, df_2])\n",
    "\n",
    "         df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
    "\n",
    "\n",
    "         Y = np.array(df_train[187].values).astype(np.int8)\n",
    "         X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "         Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "         X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "         return X,Y,X_test,Y_test\n",
    "\n",
    "     else:\n",
    "         raise NotImplementedError('wrong dataset name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('mitbih')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_signals(signals: np.ndarray, target_length: int) -> np.ndarray:\n",
    "    return np.pad(signals, [(0,0), (0, target_length - signals.shape[1]), (0,0)])\n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, input_shape: Tuple[int, int], latent_dim: int):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        ## Encoder ##\n",
    "        conv = layers.Conv1D(filters=5, kernel_size=3, strides=2, padding='same', input_shape=input_shape)\n",
    "        max_pool = layers.MaxPool1D(pool_size=2, padding='same')\n",
    "        conv2 = layers.Conv1D(filters=15, kernel_size=3, strides=2, padding='same')\n",
    "        max_pool2 = layers.MaxPool1D(pool_size=2, padding='same')\n",
    "        conv3 = layers.Conv1D(filters=30, kernel_size=3, strides=2, padding='same')\n",
    "        max_pool3 = layers.MaxPool1D(pool_size=2, padding='same')\n",
    "        flatten = layers.Flatten()\n",
    "        dense = layers.Dense(latent_dim)\n",
    "\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            conv,\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            max_pool,\n",
    "            conv2,\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            max_pool2,\n",
    "            conv3,\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Dropout(rate=0.1),\n",
    "            max_pool3,\n",
    "            flatten,\n",
    "            dense\n",
    "        ])\n",
    "\n",
    "        ## Decoder ##\n",
    "        dec_dense = layers.Dense(90, input_shape=(latent_dim,))\n",
    "        reshape = layers.Reshape((3, 30))\n",
    "        convt3 = layers.Conv1DTranspose(filters=30, kernel_size=3, strides=2, padding='same')\n",
    "        upsample3 = layers.UpSampling1D(size=2)\n",
    "        convt2 = layers.Conv1DTranspose(filters=15, kernel_size=3, strides=2, padding='same')\n",
    "        upsample2 = layers.UpSampling1D(size=2)\n",
    "        convt = layers.Conv1DTranspose(filters=5, kernel_size=3, strides=2, padding='same')\n",
    "        upsample = layers.UpSampling1D(size=2)\n",
    "        convt_final = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=1, padding='same')\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            dec_dense,\n",
    "            layers.ReLU(),\n",
    "            reshape,\n",
    "            convt3,\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            upsample3,\n",
    "            convt2,\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            upsample2,\n",
    "            convt,\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            upsample,\n",
    "            convt_final\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the signals to a large enough multiple of 2 so that there is no loss\n",
    "# in dimensionality through the autoencoder.\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size = 0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_valid_padded = pad_signals(X_valid_split, 192)\n",
    "X_train_padded = pad_signals(X_train_split, 192)\n",
    "\n",
    "autoencoder = Autoencoder(input_shape=X_train_padded.shape[1:], latent_dim=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "    loss=losses.MeanSquaredError()\n",
    ")\n",
    "autoencoder.fit(\n",
    "    X_train_padded,\n",
    "    X_train_padded,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid_padded, X_valid_padded)\n",
    ")\n",
    "autoencoder.save_weights('./checkpoints/autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(input_shape=X_train_padded.shape[1:], latent_dim=45)\n",
    "autoencoder.load_weights('./checkpoints/autoencoder')\n",
    "\n",
    "signal = X_train_padded[1142][tf.newaxis, ...]\n",
    "print(f'Signal shape: {signal.shape}')\n",
    "\n",
    "encoded_signal = autoencoder.encode(signal)\n",
    "print(f'Encoded signal shape: {encoded_signal.shape}')\n",
    "\n",
    "reconstructed_signal = autoencoder(signal)\n",
    "print(f'Reconstructed signal shape: {reconstructed_signal.shape}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(18,6))\n",
    "\n",
    "ax.plot(tf.squeeze(signal, axis=0), label='Original Signal')\n",
    "ax.plot(tf.squeeze(reconstructed_signal, axis=0), label='Reconstructed Signal')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Preliminary accuracy result on `mitbih` dataset:\n",
    "\n",
    "| Latent Rep. | Classifier | Accuracy |\n",
    "|---|---|---|\n",
    "| Autoencoder (45D) | SVM (no fine-tune) | 0.9680201016503912 |\n",
    "| Autoencoder (45D) | GBC (no fine-tune) | 0.9479755582205471 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = autoencoder.encode(X_train_padded)\n",
    "X_valid_transformed = autoencoder.encode(X_valid_padded)\n",
    "\n",
    "svm = SVC(verbose=True)\n",
    "\n",
    "svm.fit(X_train_transformed, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.score(X_valid_transformed, y_valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = autoencoder.encode(X_train_padded)\n",
    "X_valid_transformed = autoencoder.encode(X_valid_padded)\n",
    "\n",
    "gbc = GradientBoostingClassifier(verbose=2)\n",
    "\n",
    "gbc.fit(X_train_transformed, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.score(X_valid_transformed, y_valid_split)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
